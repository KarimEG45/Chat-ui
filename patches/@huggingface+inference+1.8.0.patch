diff --git a/node_modules/@huggingface/inference/dist/index.d.ts b/node_modules/@huggingface/inference/dist/index.d.ts
index edeec88..113e63c 100644
--- a/node_modules/@huggingface/inference/dist/index.d.ts
+++ b/node_modules/@huggingface/inference/dist/index.d.ts
@@ -19,9 +19,16 @@ interface Options {
      * (Default: false) Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places.
      */
     wait_for_model?: boolean;
+    /**
+     * Custom headers to send with the request.
+     */
+    headers?: Record<string, string>;
 }
 interface Args {
-    model: string;
+    /**
+     * The model to use. Optional for endpoints.
+     */
+    model?: string;
 }
 type FillMaskArgs = Args & {
     inputs: string;
@@ -414,6 +421,19 @@ interface ConversationalReturn {
     warnings: string[];
 }
 type FeatureExtractionArgs = Args & {
+    /**
+     *  The inputs is a string or a list of strings to get the features from.
+     *
+     *  inputs: "That is a happy person",
+     *
+     */
+    inputs: string | string[];
+};
+/**
+ * Returned values are a list of floats, or a list of list of floats (depending on if you sent a string or a list of string, and if the automatic reduction, usually mean_pooling for instance was applied for you or not. This should be explained on the model's README.
+ */
+type FeatureExtractionReturn = (number | number[])[];
+type SentenceSimiliarityArgs = Args & {
     /**
      * The inputs vary based on the model. For example when using sentence-transformers/paraphrase-xlm-r-multilingual-v1 the inputs will look like this:
      *
@@ -425,9 +445,9 @@ type FeatureExtractionArgs = Args & {
     inputs: Record<string, unknown> | Record<string, unknown>[];
 };
 /**
- * Returned values are a list of floats, or a list of list of floats (depending on if you sent a string or a list of string, and if the automatic reduction, usually mean_pooling for instance was applied for you or not. This should be explained on the model's README.
+ * Returned values are a list of floats
  */
-type FeatureExtractionReturn = (number | number[])[];
+type SentenceSimiliarityReturn = number[];
 type ImageClassificationArgs = Args & {
     /**
      * Binary image data
@@ -565,7 +585,12 @@ interface ImageToTextReturn {
 declare class HfInference {
     private readonly apiKey;
     private readonly defaultOptions;
-    constructor(apiKey?: string, defaultOptions?: Options);
+    private readonly endpointUrl?;
+    constructor(apiKey?: string, defaultOptions?: Options, endpointUrl?: string);
+    /**
+     * Returns copy of HfInference tied to a specified endpoint.
+     */
+    endpoint(endpointUrl: string): HfInference;
     /**
      * Tries to fill in a hole with a missing word (token to be precise). That’s the base task for BERT models.
      */
@@ -615,6 +640,10 @@ declare class HfInference {
      * This task reads some text and outputs raw float values, that are usually consumed as part of a semantic database/semantic search.
      */
     featureExtraction(args: FeatureExtractionArgs, options?: Options): Promise<FeatureExtractionReturn>;
+    /**
+     * Calculate the semantic similarity between one text and a list of other sentences by comparing their embeddings.
+     */
+    sentenceSimiliarity(args: SentenceSimiliarityArgs, options?: Options): Promise<SentenceSimiliarityReturn>;
     /**
      * This task reads some audio input and outputs the said words within the audio files.
      * Recommended model (english language): facebook/wav2vec2-large-960h-lv60-self
@@ -674,4 +703,4 @@ declare class HfInference {
     }): AsyncGenerator<T>;
 }
 
-export { Args, AudioClassificationArgs, AudioClassificationReturn, AudioClassificationReturnValue, AutomaticSpeechRecognitionArgs, AutomaticSpeechRecognitionReturn, ConversationalArgs, ConversationalReturn, FeatureExtractionArgs, FeatureExtractionReturn, FillMaskArgs, FillMaskReturn, HfInference, ImageClassificationArgs, ImageClassificationReturn, ImageClassificationReturnValue, ImageSegmentationArgs, ImageSegmentationReturn, ImageSegmentationReturnValue, ImageToTextArgs, ImageToTextReturn, ObjectDetectionArgs, ObjectDetectionReturn, ObjectDetectionReturnValue, Options, QuestionAnswerArgs, QuestionAnswerReturn, SummarizationArgs, SummarizationReturn, TableQuestionAnswerArgs, TableQuestionAnswerReturn, TextClassificationArgs, TextClassificationReturn, TextGenerationArgs, TextGenerationReturn, TextGenerationStreamBestOfSequence, TextGenerationStreamDetails, TextGenerationStreamFinishReason, TextGenerationStreamPrefillToken, TextGenerationStreamReturn, TextGenerationStreamToken, TextToImageArgs, TextToImageReturn, TokenClassificationArgs, TokenClassificationReturn, TokenClassificationReturnValue, TranslationArgs, TranslationReturn, ZeroShotClassificationArgs, ZeroShotClassificationReturn, ZeroShotClassificationReturnValue };
+export { Args, AudioClassificationArgs, AudioClassificationReturn, AudioClassificationReturnValue, AutomaticSpeechRecognitionArgs, AutomaticSpeechRecognitionReturn, ConversationalArgs, ConversationalReturn, FeatureExtractionArgs, FeatureExtractionReturn, FillMaskArgs, FillMaskReturn, HfInference, ImageClassificationArgs, ImageClassificationReturn, ImageClassificationReturnValue, ImageSegmentationArgs, ImageSegmentationReturn, ImageSegmentationReturnValue, ImageToTextArgs, ImageToTextReturn, ObjectDetectionArgs, ObjectDetectionReturn, ObjectDetectionReturnValue, Options, QuestionAnswerArgs, QuestionAnswerReturn, SentenceSimiliarityArgs, SentenceSimiliarityReturn, SummarizationArgs, SummarizationReturn, TableQuestionAnswerArgs, TableQuestionAnswerReturn, TextClassificationArgs, TextClassificationReturn, TextGenerationArgs, TextGenerationReturn, TextGenerationStreamBestOfSequence, TextGenerationStreamDetails, TextGenerationStreamFinishReason, TextGenerationStreamPrefillToken, TextGenerationStreamReturn, TextGenerationStreamToken, TextToImageArgs, TextToImageReturn, TokenClassificationArgs, TokenClassificationReturn, TokenClassificationReturnValue, TranslationArgs, TranslationReturn, ZeroShotClassificationArgs, ZeroShotClassificationReturn, ZeroShotClassificationReturnValue };
diff --git a/node_modules/@huggingface/inference/dist/index.js b/node_modules/@huggingface/inference/dist/index.js
index c6b6270..e56d122 100644
--- a/node_modules/@huggingface/inference/dist/index.js
+++ b/node_modules/@huggingface/inference/dist/index.js
@@ -143,9 +143,17 @@ var TextGenerationStreamFinishReason = /* @__PURE__ */ ((TextGenerationStreamFin
 var HfInference = class {
   apiKey;
   defaultOptions;
-  constructor(apiKey = "", defaultOptions = {}) {
+  endpointUrl;
+  constructor(apiKey = "", defaultOptions = {}, endpointUrl) {
     this.apiKey = apiKey;
     this.defaultOptions = defaultOptions;
+    this.endpointUrl = endpointUrl;
+  }
+  /**
+   * Returns copy of HfInference tied to a specified endpoint.
+   */
+  endpoint(endpointUrl) {
+    return new HfInference(this.apiKey, this.defaultOptions, endpointUrl);
   }
   /**
    * Tries to fill in a hole with a missing word (token to be precise). That’s the base task for BERT models.
@@ -289,6 +297,36 @@ var HfInference = class {
    */
   async featureExtraction(args, options) {
     const res = await this.request(args, options);
+    let isValidOutput = true;
+    if (Array.isArray(res)) {
+      for (const e of res) {
+        if (Array.isArray(e)) {
+          isValidOutput = e.every((x) => typeof x === "number");
+          if (!isValidOutput) {
+            break;
+          }
+        } else if (typeof e !== "number") {
+          isValidOutput = false;
+          break;
+        }
+      }
+    } else {
+      isValidOutput = false;
+    }
+    if (!isValidOutput) {
+      throw new TypeError("Invalid inference output: output must be of type Array<Array<number> | number>");
+    }
+    return res;
+  }
+  /**
+   * Calculate the semantic similarity between one text and a list of other sentences by comparing their embeddings.
+   */
+  async sentenceSimiliarity(args, options) {
+    const res = await this.request(args, options);
+    const isValidOutput = Array.isArray(res) && res.every((x) => typeof x === "number");
+    if (!isValidOutput) {
+      throw new TypeError("Invalid inference output: output must be of type Array<number>");
+    }
     return res;
   }
   /**
@@ -402,7 +440,7 @@ var HfInference = class {
   makeRequestOptions(args, options) {
     const mergedOptions = { ...this.defaultOptions, ...options };
     const { model, ...otherArgs } = args;
-    const headers = {};
+    let headers = {};
     if (this.apiKey) {
       headers["Authorization"] = `Bearer ${this.apiKey}`;
     }
@@ -420,7 +458,11 @@ var HfInference = class {
         headers["X-Load-Model"] = "0";
       }
     }
-    const url = `${HF_INFERENCE_API_BASE_URL}${model}`;
+    headers = { ...headers, ...this.defaultOptions?.headers };
+    if (!model && !this.endpointUrl) {
+      throw new Error("Model is required for Inference API");
+    }
+    const url = this.endpointUrl ? this.endpointUrl : `${HF_INFERENCE_API_BASE_URL}${model}`;
     const info = {
       headers,
       method: "POST",
@@ -443,6 +485,12 @@ var HfInference = class {
     }
     if (options?.blob) {
       if (!response.ok) {
+        if (response.headers.get("Content-Type")?.startsWith("application/json")) {
+          const output2 = await response.json();
+          if (output2.error) {
+            throw new Error(output2.error);
+          }
+        }
         throw new Error("An error occurred while fetching the blob");
       }
       return await response.blob();
diff --git a/node_modules/@huggingface/inference/dist/index.mjs b/node_modules/@huggingface/inference/dist/index.mjs
index 1a4d59e..2c9f77e 100644
--- a/node_modules/@huggingface/inference/dist/index.mjs
+++ b/node_modules/@huggingface/inference/dist/index.mjs
@@ -116,9 +116,17 @@ var TextGenerationStreamFinishReason = /* @__PURE__ */ ((TextGenerationStreamFin
 var HfInference = class {
   apiKey;
   defaultOptions;
-  constructor(apiKey = "", defaultOptions = {}) {
+  endpointUrl;
+  constructor(apiKey = "", defaultOptions = {}, endpointUrl) {
     this.apiKey = apiKey;
     this.defaultOptions = defaultOptions;
+    this.endpointUrl = endpointUrl;
+  }
+  /**
+   * Returns copy of HfInference tied to a specified endpoint.
+   */
+  endpoint(endpointUrl) {
+    return new HfInference(this.apiKey, this.defaultOptions, endpointUrl);
   }
   /**
    * Tries to fill in a hole with a missing word (token to be precise). That’s the base task for BERT models.
@@ -262,6 +270,36 @@ var HfInference = class {
    */
   async featureExtraction(args, options) {
     const res = await this.request(args, options);
+    let isValidOutput = true;
+    if (Array.isArray(res)) {
+      for (const e of res) {
+        if (Array.isArray(e)) {
+          isValidOutput = e.every((x) => typeof x === "number");
+          if (!isValidOutput) {
+            break;
+          }
+        } else if (typeof e !== "number") {
+          isValidOutput = false;
+          break;
+        }
+      }
+    } else {
+      isValidOutput = false;
+    }
+    if (!isValidOutput) {
+      throw new TypeError("Invalid inference output: output must be of type Array<Array<number> | number>");
+    }
+    return res;
+  }
+  /**
+   * Calculate the semantic similarity between one text and a list of other sentences by comparing their embeddings.
+   */
+  async sentenceSimiliarity(args, options) {
+    const res = await this.request(args, options);
+    const isValidOutput = Array.isArray(res) && res.every((x) => typeof x === "number");
+    if (!isValidOutput) {
+      throw new TypeError("Invalid inference output: output must be of type Array<number>");
+    }
     return res;
   }
   /**
@@ -375,7 +413,7 @@ var HfInference = class {
   makeRequestOptions(args, options) {
     const mergedOptions = { ...this.defaultOptions, ...options };
     const { model, ...otherArgs } = args;
-    const headers = {};
+    let headers = {};
     if (this.apiKey) {
       headers["Authorization"] = `Bearer ${this.apiKey}`;
     }
@@ -393,7 +431,11 @@ var HfInference = class {
         headers["X-Load-Model"] = "0";
       }
     }
-    const url = `${HF_INFERENCE_API_BASE_URL}${model}`;
+    headers = { ...headers, ...this.defaultOptions?.headers };
+    if (!model && !this.endpointUrl) {
+      throw new Error("Model is required for Inference API");
+    }
+    const url = this.endpointUrl ? this.endpointUrl : `${HF_INFERENCE_API_BASE_URL}${model}`;
     const info = {
       headers,
       method: "POST",
@@ -416,6 +458,12 @@ var HfInference = class {
     }
     if (options?.blob) {
       if (!response.ok) {
+        if (response.headers.get("Content-Type")?.startsWith("application/json")) {
+          const output2 = await response.json();
+          if (output2.error) {
+            throw new Error(output2.error);
+          }
+        }
         throw new Error("An error occurred while fetching the blob");
       }
       return await response.blob();
diff --git a/node_modules/@huggingface/inference/src/HfInference.ts b/node_modules/@huggingface/inference/src/HfInference.ts
index bbf7041..1d51296 100644
--- a/node_modules/@huggingface/inference/src/HfInference.ts
+++ b/node_modules/@huggingface/inference/src/HfInference.ts
@@ -26,10 +26,18 @@ export interface Options {
 	 * (Default: false) Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places.
 	 */
 	wait_for_model?: boolean;
+
+	/**
+	 * Custom headers to send with the request.
+	 */
+	headers?: Record<string, string>;
 }
 
 export interface Args {
-	model: string;
+	/**
+	 * The model to use. Optional for endpoints.
+	 */
+	model?: string;
 }
 
 export type FillMaskArgs = Args & {
@@ -449,8 +457,22 @@ export interface ConversationalReturn {
 	generated_text: string;
 	warnings: string[];
 }
-
 export type FeatureExtractionArgs = Args & {
+	/**
+	 *  The inputs is a string or a list of strings to get the features from.
+	 *
+	 *  inputs: "That is a happy person",
+	 *
+	 */
+	inputs: string | string[];
+};
+
+/**
+ * Returned values are a list of floats, or a list of list of floats (depending on if you sent a string or a list of string, and if the automatic reduction, usually mean_pooling for instance was applied for you or not. This should be explained on the model's README.
+ */
+export type FeatureExtractionReturn = (number | number[])[];
+
+export type SentenceSimiliarityArgs = Args & {
 	/**
 	 * The inputs vary based on the model. For example when using sentence-transformers/paraphrase-xlm-r-multilingual-v1 the inputs will look like this:
 	 *
@@ -463,9 +485,9 @@ export type FeatureExtractionArgs = Args & {
 };
 
 /**
- * Returned values are a list of floats, or a list of list of floats (depending on if you sent a string or a list of string, and if the automatic reduction, usually mean_pooling for instance was applied for you or not. This should be explained on the model's README.
+ * Returned values are a list of floats
  */
-export type FeatureExtractionReturn = (number | number[])[];
+export type SentenceSimiliarityReturn = number[];
 
 export type ImageClassificationArgs = Args & {
 	/**
@@ -625,10 +647,19 @@ export interface ImageToTextReturn {
 export class HfInference {
 	private readonly apiKey: string;
 	private readonly defaultOptions: Options;
+	private readonly endpointUrl?: string;
 
-	constructor(apiKey = "", defaultOptions: Options = {}) {
+	constructor(apiKey = "", defaultOptions: Options = {}, endpointUrl?: string) {
 		this.apiKey = apiKey;
 		this.defaultOptions = defaultOptions;
+		this.endpointUrl = endpointUrl;
+	}
+
+	/**
+	 * Returns copy of HfInference tied to a specified endpoint.
+	 */
+	public endpoint(endpointUrl: string): HfInference {
+		return new HfInference(this.apiKey, this.defaultOptions, endpointUrl);
 	}
 
 	/**
@@ -834,6 +865,44 @@ export class HfInference {
 	 */
 	public async featureExtraction(args: FeatureExtractionArgs, options?: Options): Promise<FeatureExtractionReturn> {
 		const res = await this.request<FeatureExtractionReturn>(args, options);
+		let isValidOutput = true;
+		// Check if output is an array
+		if (Array.isArray(res)) {
+			for (const e of res) {
+				// Check if output is an array of arrays or numbers
+				if (Array.isArray(e)) {
+					// if all elements are numbers, continue
+					isValidOutput = e.every((x) => typeof x === "number");
+					if (!isValidOutput) {
+						break;
+					}
+				} else if (typeof e !== "number") {
+					isValidOutput = false;
+					break;
+				}
+			}
+		} else {
+			isValidOutput = false;
+		}
+		if (!isValidOutput) {
+			throw new TypeError("Invalid inference output: output must be of type Array<Array<number> | number>");
+		}
+		return res;
+	}
+
+	/**
+	 * Calculate the semantic similarity between one text and a list of other sentences by comparing their embeddings.
+	 */
+	public async sentenceSimiliarity(
+		args: SentenceSimiliarityArgs,
+		options?: Options
+	): Promise<SentenceSimiliarityReturn> {
+		const res = await this.request<SentenceSimiliarityReturn>(args, options);
+
+		const isValidOutput = Array.isArray(res) && res.every((x) => typeof x === "number");
+		if (!isValidOutput) {
+			throw new TypeError("Invalid inference output: output must be of type Array<number>");
+		}
 		return res;
 	}
 
@@ -990,7 +1059,8 @@ export class HfInference {
 		const mergedOptions = { ...this.defaultOptions, ...options };
 		const { model, ...otherArgs } = args;
 
-		const headers: Record<string, string> = {};
+		let headers: Record<string, string> = {};
+
 		if (this.apiKey) {
 			headers["Authorization"] = `Bearer ${this.apiKey}`;
 		}
@@ -1011,7 +1081,12 @@ export class HfInference {
 			}
 		}
 
-		const url = `${HF_INFERENCE_API_BASE_URL}${model}`;
+		headers = { ...headers, ...this.defaultOptions?.headers };
+
+		if (!model && !this.endpointUrl) {
+			throw new Error("Model is required for Inference API");
+		}
+		const url = this.endpointUrl ? this.endpointUrl : `${HF_INFERENCE_API_BASE_URL}${model}`;
 		const info: RequestInit = {
 			headers,
 			method: "POST",
@@ -1048,6 +1123,12 @@ export class HfInference {
 
 		if (options?.blob) {
 			if (!response.ok) {
+				if (response.headers.get("Content-Type")?.startsWith("application/json")) {
+					const output = await response.json();
+					if (output.error) {
+						throw new Error(output.error);
+					}
+				}
 				throw new Error("An error occurred while fetching the blob");
 			}
 			return (await response.blob()) as T;
